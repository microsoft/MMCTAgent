{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c13ffc",
   "metadata": {},
   "source": [
    "## Example Notebook: Using VideoAgent\n",
    "\n",
    "### üõ†Ô∏è Setup Instructions\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "- Ensure you have created an `.env` file in the **same directory** as this notebook. It must contain all required environment variables (e.g., API keys or configuration values expected by `VideoAgent`).\n",
    "- Make sure all required libraries are installed by running:\n",
    "\n",
    "  ```bash\n",
    "  pip install -r requirements.txt\n",
    "  ```\n",
    "\n",
    "### About\n",
    "\n",
    "**VideoAgent** operates in two key stages:\n",
    "\n",
    "1. **Video Retrieval** ‚Äì Given a user query, the agent retrieves relevant videos from a pre-ingested **Azure AI Search index**. This search ensures that only contextually relevant videos are passed on for deep analysis.\n",
    "\n",
    "2. **Video Question Answering (QA)** ‚Äì After retrieval, the agent uses the **Multi-Modal Critical Thinking (MMCT)** framework ([arxiv.org/abs/2405.18358](https://arxiv.org/abs/2405.18358)) to generate a high-quality answer. MMCT involves two agents:\n",
    "\n",
    "   - **Planner**: Drives the reasoning process using a structured toolchain, generating an initial response.\n",
    "   - **Critic (optional)**: Analyzes the planner's output and, if needed, provides feedback that prompts an improved final answer.\n",
    "\n",
    "> **Note:** The critic agent is enabled by default. You can disable it by setting `use_critic_agent=False` during initialization.  \n",
    "> **Disabling the critic agent skips the feedback loop and may reduce the accuracy of the final response.**\n",
    "\n",
    "---\n",
    "\n",
    "### Tool Workflow\n",
    "\n",
    "Unlike independent tool selection, **VideoAgent uses a fixed pipeline** of tools that work collaboratively during the QA stage. These tools are automatically orchestrated by the planner:\n",
    "\n",
    "- `GET_VIDEO_DESCRIPTION` ‚Äì Extracts the full transcript and a high-level visual summary of the video.\n",
    "- `QUERY_VIDEO_DESCRIPTION` ‚Äì Given a query, this tool identifies **three timestamps** in the transcript that are most relevant.\n",
    "- `QUERY_FRAMES_COMPUTER_VISION` _(optional)_ ‚Äì Uses **Computer Vision** to return **three additional timestamps** related to the visual content of the query.\n",
    "- `QUERY_VISION_LLM` ‚Äì Uses **vision LLM** to inspect video frames around the identified timestamps and generate a detailed response grounded in both visual and textual understanding.\n",
    "\n",
    "By default, all tools are used in a coordinated pipeline. You can disable **only** the Computer Vision tool by setting `use_computer_vision_tool=False` during agent initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128f1ba",
   "metadata": {},
   "source": [
    "### Importing Libaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb122cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmct.video_pipeline import VideoAgent\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the configuration first\n",
    "try:\n",
    "    from mmct.config.settings import MMCTConfig\n",
    "    config = MMCTConfig()\n",
    "    print(\"‚úÖ Configuration loaded successfully\")\n",
    "    print(f\"LLM Provider: {config.llm.provider}\")\n",
    "    print(f\"LLM Endpoint: {config.llm.endpoint}\")\n",
    "    print(f\"LLM Deployment: {config.llm.deployment_name}\")\n",
    "    print(f\"Embedding Provider: {config.embedding.provider}\")\n",
    "    print(f\"Embedding Endpoint: {config.embedding.endpoint}\")\n",
    "    print(f\"Embedding Deployment: {config.embedding.deployment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Create VideoAgent instance\n",
    "video_agent = VideoAgent(\n",
    "    query=\"\",\n",
    "    index_name=\"\",\n",
    "    top_n=2,\n",
    "    use_computer_vision_tool=False,\n",
    "    use_critic_agent=True,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "response = await video_agent()\n",
    "print(\"VideoAgent executed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8401d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the response\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmct_opensource",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
