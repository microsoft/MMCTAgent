"""
This tool can do the visual criticism on the provided logs containing reasoning and query.
"""

# Importing Libraries
import os
import json
import asyncio
from typing_extensions import Annotated
from mmct.video_pipeline.prompts_and_description import get_critic_tool_system_prompt
from mmct.providers.factory import provider_factory

from dotenv import load_dotenv, find_dotenv

# Load environment variables
load_dotenv(find_dotenv(), override=True)

llm_provider = provider_factory.create_llm_provider()




async def critic_tool(
    user_query: Annotated[str, "The original user question or query that needs to be answered"],
    answer: Annotated[str, "The complete draft response generated by the Planner agent to answer the user query"],
    raw_context: Annotated[str, "Detailed context information retrieved from tools (get_context, get_relevant_frames, etc.) that was used to generate the answer. Include all relevant data, evidence, and source information"],
    reasoning_steps: Annotated[str, "Step-by-step reasoning process and logical flow that the Planner followed to arrive at the final answer, including decision points and justifications"],
):
    """
    Critique tool used by the Critic Agent to evaluate the correctness, coherence, and quality of
    the planner response, based only on reasoning logs without visual verification.

    Parameters:
    - user_query (str): The exact query from the user
    - answer (str): The draft answer proposed by the Planner agent
    - raw_context (str): The context retrieved/output from tools to prepare the draft answer
    - reasoning_steps (str): Ordered reasoning steps showing how the Planner arrived at the draft answer

    Returns:
    - dict: Structured critique response containing evaluation and recommendations
    """
    try:
        # Create structured logs from individual parameters
        logs = {
            "user_query": user_query,
            "answer": answer,
            "raw_context": raw_context,
            "reasoning_steps": reasoning_steps
        }

        # Convert logs to JSON string for the prompt
        logs_json = json.dumps(logs, indent=2)

        payload = {
            "messages": [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": await get_critic_tool_system_prompt(),
                        }
                    ],
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"These are the logs: {logs_json}"}
                    ]
                },
            ],
           "temperature": 0,
           "top_p": 0.1,
        }

        retry_intervals = [10, 15]
        for attempt, wait_time in enumerate(retry_intervals, start=1):
            try:
                result = await llm_provider.chat_completion(
                    messages=payload["messages"],
                    temperature=payload["temperature"],
                    top_p=payload["top_p"]
                )
                response = type('obj', (object,), {
                    'choices': [type('obj', (object,), {
                        'message': type('obj', (object,), {'content': result['content']})()
                    })()]
                })()
                break
            except Exception as e:
                if attempt < len(retry_intervals):
                    await asyncio.sleep(wait_time)
                else:
                    return f"Final attempt failed: {e}"

        response_content = json.loads(response.choices[0].message.content)
        return response_content
    except Exception as e:
        raise Exception(e)


if __name__ == "__main__":
    # Example usage - replace with your actual values
    user_query = "user-query"
    answer = "answer-to-the-user-query"
    raw_context = "raw-context-used-to-generate-the-answer"
    reasoning_steps = "step-by-step-reasoning-logs"

    res = asyncio.run(
        critic_tool(
            user_query=user_query,
            answer=answer,
            raw_context=raw_context,
            reasoning_steps=reasoning_steps
        )
    )
    print(res)
