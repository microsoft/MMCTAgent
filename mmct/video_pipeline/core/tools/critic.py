"""
This tool can do the visual criticism on the provided logs containing reasoning and query.
"""

# Importing Libraries
import os
import json
import asyncio
from typing_extensions import Annotated
from mmct.video_pipeline.prompts_and_description import get_critic_tool_system_prompt
from mmct.llm_client import LLMClient

from dotenv import load_dotenv, find_dotenv

# Load environment variables
load_dotenv(find_dotenv(), override=True)

service_provider = os.getenv("LLM_PROVIDER", "azure")
openai_client = LLMClient(service_provider=service_provider, isAsync=True)
openai_client = openai_client.get_client()




async def critic_tool(
    user_query: Annotated[str, "The original user question or query that needs to be answered"],
    answer: Annotated[str, "The complete draft response generated by the Planner agent to answer the user query"],
    raw_context: Annotated[str, "Detailed context information retrieved from tools (get_context, get_relevant_frames, etc.) that was used to generate the answer. Include all relevant data, evidence, and source information"],
    reasoning_steps: Annotated[str, "Step-by-step reasoning process and logical flow that the Planner followed to arrive at the final answer, including decision points and justifications"],
):
    """
    Critique tool used by the Critic Agent to evaluate the correctness, coherence, and quality of
    the planner response, based only on reasoning logs without visual verification.

    Parameters:
    - user_query (str): The exact query from the user
    - answer (str): The draft answer proposed by the Planner agent
    - raw_context (str): The context retrieved/output from tools to prepare the draft answer
    - reasoning_steps (str): Ordered reasoning steps showing how the Planner arrived at the draft answer

    Returns:
    - dict: Structured critique response containing evaluation and recommendations
    """
    try:
        # Create structured logs from individual parameters
        logs = {
            "user_query": user_query,
            "answer": answer,
            "raw_context": raw_context,
            "reasoning_steps": reasoning_steps
        }

        # Convert logs to JSON string for the prompt
        logs_json = json.dumps(logs, indent=2)

        payload = {
            "messages": [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": await get_critic_tool_system_prompt(),
                        }
                    ],
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": f"These are the logs: {logs_json}"}
                    ]
                },
            ],
           "temperature": 0,
           "top_p": 0.1,
        }

        retry_intervals = [10, 15]
        for attempt, wait_time in enumerate(retry_intervals, start=1):
            try:
                response = await openai_client.chat.completions.create(
                    model=os.getenv(
                        "LLM_DEPLOYMENT_NAME"
                        if os.getenv("LLM_PROVIDER") == "azure"
                        else "OPENAI_MODEL_NAME"
                    ),
                    temperature=payload["temperature"],
                    messages=payload["messages"],
                    top_p=payload["top_p"],
                )
                break
            except Exception as e:
                if attempt < len(retry_intervals):
                    await asyncio.sleep(wait_time)
                else:
                    return f"Final attempt failed: {e}"

        response_content = json.loads(response.choices[0].message.content)
        return response_content
    except Exception as e:
        raise Exception(e)


if __name__ == "__main__":
    # Example usage - replace with your actual values
    user_query = "What farming techniques are shown in the video?"
    answer = "The video shows traditional farming methods including crop rotation and manual harvesting."
    raw_context = "Retrieved context from frames showing farmers working in fields with traditional tools."
    reasoning_steps = "1. Analyzed video frames 2. Identified farming activities 3. Summarized techniques observed"

    res = asyncio.run(
        critic_tool(
            user_query=user_query,
            answer=answer,
            raw_context=raw_context,
            reasoning_steps=reasoning_steps
        )
    )
    print(res)
