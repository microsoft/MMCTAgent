"""
This tool can do the visual criticism on the provided logs containing reasoning and query.
"""

# Importing Libaries
import os
import json
import numpy as np
import asyncio
from typing_extensions import Annotated
from mmct.video_pipeline.prompts_and_description import get_critic_tool_system_prompt
from mmct.llm_client import LLMClient
from mmct.video_pipeline.utils.helper import (
    download_blobs,
    encode_image_to_base64,
    stack_images_horizontally,
    load_images,
    get_media_folder
)

from dotenv import load_dotenv, find_dotenv

# Load environment variables
load_dotenv(find_dotenv(), override=True)

service_provider = os.getenv("LLM_PROVIDER", "azure")
openai_client = LLMClient(service_provider=service_provider, isAsync=True)
openai_client = openai_client.get_client()




async def critic_tool(
    logs: Annotated[
        str,
        "A structured string representing the agentic reasoning workflow. This includes:\n"
        "- user_query: The original user query that initiated the workflow.\n"
        "- answer: The answer generated by the planner agent.\n"
        "- raw_context: context that were used while curating the planner response. it should be as it is from the context tool.\n"
        "- reasoning_steps_detailed: A chronological chain of reasoning steps taken by the agents.\n"
        "- tool_invocation_records:Tool invocation records (tool name, inputs, and outputs).\n"
        "- Intermediate observations and reflections.\n"
        "This log serves as a trace of how the final output was produced, useful for critique and debugging.",
    ],
):
    """
    V2 critique tool used by the Critic Agent to evaluate the correctness, coherence, and quality of
    the planner response, based only on reasoning logs without visual verification.

    Parameters:
    - logs (str): A comprehensive, structured log of the agent's reasoning and workflow chain.
      This should include:
        * The initial user query.
        * A sequence of reasoning steps performed by the agent.
        * Each tool used, along with input parameters and their respective outputs.
        * Any intermediate insights or reflections.
      This log enables detailed critique of the decision-making and output-generation process.
    """
    try:
        payload = {
            "messages": [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": await get_critic_tool_system_prompt(),
                        }
                    ],
                },
                {
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": f"These are the logs: {logs}"}
                    ]
                },
            ],
           "temperature": 0,
           "top_p": 0.1,
        }

        retry_intervals = [10, 15]
        for attempt, wait_time in enumerate(retry_intervals, start=1):
            try:
                response = await openai_client.chat.completions.create(
                    model=os.getenv(
                        "LLM_DEPLOYMENT_NAME"
                        if os.getenv("LLM_PROVIDER") == "azure"
                        else "OPENAI_MODEL_NAME"
                    ),
                    temperature=payload["temperature"],
                    messages=payload["messages"],
                    top_p=payload["top_p"],
                )
                break
            except Exception as e:
                if attempt < len(retry_intervals):
                    await asyncio.sleep(wait_time)
                else:
                    return f"Final attempt failed: {e}"

        response_content = json.loads(response.choices[0].message.content)
        return json.dumps(
            {"Critic Feedback": response_content.get("Feedback", "")}
        ), response_content.get("Verdict", "YES")
    except Exception as e:
        raise Exception(e)


if __name__ == "__main__":
    # Example usage - replace with your actual values
    timestamps_predicted = "00:00:10|00:00:30|00:01:00|00:01:30|00:02:00|00:02:30"
    video_id = "example_video_id_hash"
    logs = "query: example question about the video, response: example response about the video content"
    use_computer_vision_tool = True
    res = asyncio.run(
        critic_tool(
            timestamps_predicted=timestamps_predicted,
            video_id=video_id,
            logs=logs,
            use_computer_vision_tool=use_computer_vision_tool,
        )
    )
    pass
